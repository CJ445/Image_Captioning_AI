{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5f1fc3-8606-4e2a-9f5f-992ebddb9569",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VGG16, ResNet50\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, add\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd9e42-57dd-4c9f-af63-81635782c2a8",
   "metadata": {},
   "source": [
    "### Step 1: Extract Features from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc189e-0bb4-4728-9cd9-5cd7383bdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path, model):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
    "    features = model.predict(image, verbose=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176813c3-9626-4d04-b0f1-dcd7020eed7f",
   "metadata": {},
   "source": [
    "#### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb989b4f-1b19-4afe-a700-2d2b07f181e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = VGG16()\n",
    "image_model = Model(inputs=image_model.inputs, outputs=image_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfd13c-fc7e-42d6-b5b0-896ed089b9d6",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Caption Data (loading a preprocessed dataset for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c5d49-f0ab-48a8-a05c-908abebd2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('captions.pkl', 'rb') as f:\n",
    "    captions_data = pickle.load(f)\n",
    "\n",
    "tokenizer = captions_data['tokenizer']\n",
    "max_length = captions_data['max_length']\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec906b-519b-4e8f-950d-6f87b9d88463",
   "metadata": {},
   "source": [
    "### Step 3: Build the Caption Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09c35c-98fd-4337-a076-e67ccc94ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dense(256, activation='relu')(inputs1)\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = LSTM(256)(se1)\n",
    "    decoder1 = add([fe1, se2])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "caption_model = define_model(vocab_size, max_length)\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1e6f2-6e8e-4a61-8a65-0eb0c3f8cc8d",
   "metadata": {},
   "source": [
    "#### Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4d7bf-1206-4a80-adf1-aa486633c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.load_weights('caption_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7737418-4606-4dcf-b5a4-019339c30ce9",
   "metadata": {},
   "source": [
    "### Step 4: Generate Captions for New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a98cb-6b31-4f0a-94c5-d90da05d11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word.get(yhat, None)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final_caption = in_text.split()\n",
    "    final_caption = final_caption[1:-1]\n",
    "    final_caption = ' '.join(final_caption)\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfe7ca-9483-4d3c-8f82-ed71f163d70a",
   "metadata": {},
   "source": [
    "#### Generate caption for a new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98105866-c195-4db9-ac17-d2ab9ba64507",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'new_image.jpg'\n",
    "photo = extract_features(image_path, image_model)\n",
    "caption = generate_caption(caption_model, tokenizer, photo, max_length)\n",
    "print('Generated Caption:', caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
